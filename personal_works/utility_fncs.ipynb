{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac31e16-c0e2-4ff9-8492-08c517ef167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import filetype\n",
    "from pathlib import Path\n",
    "import nbimporter\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a962424-be52-4fd0-ae76-bdfbb4beb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradio_chat_launcher(chat):\n",
    "    return gr.ChatInterface(fn=chat, type=\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec466856-86ef-438d-87db-231221122377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_new_chat_interface(chat):\n",
    "    with gr.Blocks() as ui:\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(height=500, type='messages')\n",
    "            image_output = gr.Image(height=500)\n",
    "        with gr.Row():\n",
    "            text_input = gr.Textbox(label='Chat with the AI assistant')\n",
    "        with gr.Row():\n",
    "            clear = gr.Button('Clear')\n",
    "\n",
    "        def do_entry(message, history):\n",
    "            history=history+[{'role':'user','content':message}]\n",
    "            return '', history\n",
    "\n",
    "        text_input.submit(do_entry, inputs=[text_input, chatbot], outputs=[text_input,chatbot]).then(\n",
    "            chat, inputs=[chatbot,text_input],outputs=[chatbot,image_output]\n",
    "        )\n",
    "        clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "    return ui\n",
    "        \n",
    "\n",
    "def get_gradio_launcher(func):\n",
    "    view = gr.Interface(\n",
    "        fn=func,\n",
    "        inputs=[gr.Textbox(label=\"Your message:\"),gr.File(label=\"Upload file\"),gr.Dropdown([\"GPT\", \"GEMINI\"], label=\"Select model\")],\n",
    "        outputs=[gr.Markdown(label=\"Response:\")],\n",
    "        flagging_mode=\"never\"\n",
    "    )\n",
    "    return view\n",
    "    #view.launch()#view.launch(server_name=\"10.75.32.62\", server_port=7866)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab79f5a-3447-4b42-9fe2-5b54db1b05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradio_multi_modal_launcher(func):\n",
    "    view = gr.Interface(\n",
    "        fn=func,\n",
    "        inputs=[gr.Textbox(label=\"Your message:\"),gr.File(label=\"Upload file\"),gr.Dropdown([\"GPT\", \"GEMINI\"], label=\"Select model\")],\n",
    "        outputs=[gr.Markdown(label=\"Response:\"),gr.Image(height=512)],\n",
    "        flagging_mode=\"never\"\n",
    "    )\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0758ece0-da24-4c24-a73c-f5c4417d65cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text/plain\n"
     ]
    }
   ],
   "source": [
    "def is_ascii(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            content = f.read()\n",
    "            content.decode('ascii')  # Will raise UnicodeDecodeError if not ASCII\n",
    "        return True\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "        \n",
    "def get_file_path_str(file_path):\n",
    "    if(isinstance(file_path,Path)):\n",
    "        file_path=file_path.as_posix()\n",
    "    return file_path \n",
    "    \n",
    "def detect_mime_type(file_path):\n",
    "    file_path=get_file_path_str(file_path)\n",
    "    if is_ascii(file_path):\n",
    "        return \"text/plain\"\n",
    "\n",
    "    kind = filetype.guess(file_path)\n",
    "    if kind:\n",
    "        return kind.mime    \n",
    "\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "print(detect_mime_type(\"C:/Projects/llm_engg/llm_engineering/requirements.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c3c2f8f-8d67-4284-b5d5-1e25b160c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileContent(fp):  \n",
    "    fp=get_file_path_str(fp)\n",
    "    with open(fp, 'r') as file:\n",
    "        file_content = file.read()\n",
    "        return file_content\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64eb6bda-79d3-4ab9-9918-b5cb8218f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_conv(conv, msg):\n",
    "    conv.append(msg)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d05c8c-fb34-45a3-afa6-5509c66ef247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gpt_prompt(qt, role):\n",
    "    \"\"\"\n",
    "    Creates a correctly formatted Python dictionary for an OpenAI API message.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"role\": role,\n",
    "        \"content\": qt\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d7d779-6bbc-4124-bb03-7567fed1e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gpt_tool_response_dict(tool, response):\n",
    "    return {\n",
    "        \"tool_call_id\" : tool.id,\n",
    "        \"role\" : \"tool\",\n",
    "        \"name\" : tool.function.name,\n",
    "        \"content\" : response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb3db95f-8b02-439a-b60a-29cfd4b201c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_conv_history(conv,sys_msg):\n",
    "    conv=[]\n",
    "    conv.append(sys_msg)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb4c980a-9522-41d4-8d59-2bd595ed7608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import get_origin, get_args, Literal\n",
    "def create_tool_from_function(func):\n",
    "    \"\"\"\n",
    "    Generates an OpenAI-compatible tool definition from a Python function.\n",
    "\n",
    "    This function leverages type hints and a structured docstring to create \n",
    "    the JSON schema required by the OpenAI API for tool-calling.\n",
    "\n",
    "    It supports multi-line function descriptions and multi-line parameter\n",
    "    descriptions.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The function to be converted into a tool.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary representing the tool in the format expected by\n",
    "              the OpenAI API.\n",
    "    \"\"\"\n",
    "    # Get the clean, un-indented docstring\n",
    "    full_docstring = inspect.getdoc(func)\n",
    "    if not full_docstring:\n",
    "        raise ValueError(\"The function must have a docstring to be used as a tool.\")\n",
    "    \n",
    "    lines = [line.strip() for line in full_docstring.strip().split('\\n')]\n",
    "    \n",
    "    # --- 1. Parse Function and Parameter Descriptions ---\n",
    "    try:\n",
    "        args_section_index = lines.index('Args:')\n",
    "    except ValueError:\n",
    "        args_section_index = len(lines)\n",
    "\n",
    "    func_description = \"\\n\".join(lines[:args_section_index]).strip()\n",
    "    \n",
    "    # Robustly parse multi-line argument descriptions\n",
    "    param_docs = {}\n",
    "    current_param_name = None\n",
    "    args_section_lines = lines[args_section_index + 1:]\n",
    "\n",
    "    for line in args_section_lines:\n",
    "        if ':' in line:\n",
    "            param_name, param_desc = line.split(':', 1)\n",
    "            param_name = param_name.split('(')[0].strip()\n",
    "            # Start a new list of description lines for this parameter\n",
    "            param_docs[param_name] = [param_desc.strip()]\n",
    "            current_param_name = param_name\n",
    "        elif current_param_name:\n",
    "            # If a line doesn't have a colon, it's a continuation of the last parameter\n",
    "            param_docs[current_param_name].append(line.strip())\n",
    "\n",
    "    # Join the multi-line descriptions back into single strings\n",
    "    for name, desc_lines in param_docs.items():\n",
    "        param_docs[name] = \"\\n\".join(desc_lines)\n",
    "\n",
    "    # --- 2. Introspect Function Signature ---\n",
    "    sig = inspect.signature(func)\n",
    "    parameters_schema = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "    type_mapping = {str: \"string\", int: \"integer\", float: \"number\", bool: \"boolean\", dict: \"object\"}\n",
    "\n",
    "    for name, param in sig.parameters.items():\n",
    "        if param.default == inspect.Parameter.empty and name != 'self':\n",
    "            parameters_schema[\"required\"].append(name)\n",
    "\n",
    "        param_info = {}\n",
    "        if get_origin(param.annotation) is Literal:\n",
    "            param_info[\"type\"] = \"string\"\n",
    "            param_info[\"enum\"] = list(get_args(param.annotation))\n",
    "        else:\n",
    "            param_info[\"type\"] = type_mapping.get(param.annotation, \"string\")\n",
    "            \n",
    "        if name in param_docs:\n",
    "            param_info[\"description\"] = param_docs[name]\n",
    "\n",
    "        parameters_schema[\"properties\"][name] = param_info\n",
    "        \n",
    "    if not parameters_schema[\"properties\"]:\n",
    "        parameters_schema = {\"type\": \"object\", \"properties\": {}}\n",
    "\n",
    "    # --- 3. Assemble Final Tool ---\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": func_description,\n",
    "            \"parameters\": parameters_schema,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8192f6d8-91ce-4534-be0a-55a7676ef46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "def get_list_of_models():\n",
    "    client = genai.Client()\n",
    "    \n",
    "    print(\"List of models that support generateContent:\\n\")\n",
    "    for m in client.models.list():\n",
    "        for action in m.supported_actions:\n",
    "            if action == \"generateContent\":\n",
    "                print(m.name)\n",
    "    print(\"List of models that support embedContent:\\n\")\n",
    "    for m in client.models.list():\n",
    "        for action in m.supported_actions:\n",
    "            if action == \"embedContent\":\n",
    "                print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb147f-3e26-4f9e-9b7a-374c099bb8af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
